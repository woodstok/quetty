= Quetty =

2 parts
== Tokenizer ==
With given input text, and a list of matching rules, 
should return output with tokens that meet the satisfy the rules

===== matching rules =====
Should they be
	- functions? < lacks configurability
		- any way of packaging go single files and dynamically loading them?
	- config json file with regex
		- lacks flexibilty for matching rule i.e extra control/valiadation that we can write via code
Currently going with individual rule functions

Start with tests



== Terminal Plugin ==
1. Capture terminal contents 
2. Send to tokenizer
3. Display Output through fzf
4. If entry selected, send the content back to terminal
5. Option to send it to vim first for editing before sending it back to terminal!!!
Currently, implement for tmux.
=== tmux ===

=== vim ===


== Installer script ==
Follow fzf pattern

== Tasks ==
- [X] Study basic TDD with go
- [X] Setup infra for tests to start failing
- [X] Add splitFunc as parameter
- [X] Add space delimited tokenizer
- [X] Add path tokenizer
- [X] Add regex tokenizer
- [X] Change tokenizers to return error
- [X] change structure to build main application
- [X] add cli arguments/switches
- [X] add tokenizer combiner ( or operation )?? Is this needed
- [X] add goldenfile tests
- [ ] add path tokenizer/tests
- [ ] add ip address tokenizer/tests
- [ ] add identifier tokenizer/tests
- [ ] parallelize tokenMgr
    - [ ] make tokenset threadsafe
